{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AlignProcessor, AlignModel\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from datasets import DatasetDict, Dataset\n",
    "from collections import defaultdict \n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import wandb\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "from src.datasets.meme_text_dataloader import get_meme_text_dataloader\n",
    "from src.utilities import *\n",
    "from src.models.align_base import align_base\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dataset and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5823,\n",
       " list,\n",
       " {'category': 'memes',\n",
       "  'img_captions': ['Person in Spider Man outfit gives a lecture on stage.',\n",
       "   'Person dressed as spider man stands in front of crowd with notes'],\n",
       "  'meme_captions': ['Meme poster is frustrated about the format of the website and is making a suggestion for improvement.'],\n",
       "  'title': 'For real though',\n",
       "  'url': 'https://i.redd.it/m16dhaqyply21.jpg',\n",
       "  'img_fname': 'memes_bpet7l.png',\n",
       "  'metaphors': [{'metaphor': 'Spider Man outfit', 'meaning': 'Meme poster'},\n",
       "   {'metaphor': 'a lecture', 'meaning': 'complaint'},\n",
       "   {'metaphor': 'spider man', 'meaning': 'Meme poster'},\n",
       "   {'metaphor': 'crowd', 'meaning': 'meme readers'}],\n",
       "  'post_id': 'bpet7l'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else  'cpu')\n",
    "image_height = 289\n",
    "image_width = 289\n",
    "seed = 42\n",
    "# Load images into a Dataset, but the pixels will be transformed into list elements, which is not efficient.\n",
    "meme_loader = get_meme_text_dataloader('memecap', (image_height, image_width))\n",
    "\n",
    "# load memes and texts. In total 873 steps\n",
    "# meme_loader.load_datasets(splits=['trainval'], from_idx=0, to_idx=100) # test or trainval\n",
    "len(meme_loader.trainval_text_data), type(meme_loader.trainval_text_data), meme_loader.trainval_text_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5823,\n",
       " list,\n",
       " {'category': 'memes',\n",
       "  'img_captions': ['The little kid looks confused and is ready to ask a snarky question'],\n",
       "  'meme_captions': [\"Meme poster trying to figure out how much sleep he'll get if they continue to watch tv.\"],\n",
       "  'title': 'All my homies are nocturnal',\n",
       "  'url': 'https://i.redd.it/hrlf4s40cst51.jpg',\n",
       "  'img_fname': 'memes_jdabfs.png',\n",
       "  'metaphors': [{'metaphor': 'The little kid', 'meaning': 'Meme poster'},\n",
       "   {'metaphor': 'looks', 'meaning': 'calculating time'}],\n",
       "  'post_id': 'jdabfs'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(seed)\n",
    "random.shuffle(meme_loader.trainval_text_data) # in-place shuffle\n",
    "len(meme_loader.trainval_text_data), type(meme_loader.trainval_text_data), meme_loader.trainval_text_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4658, 1165)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_point = int(len(meme_loader.trainval_text_data) * 0.8)\n",
    "train_text = meme_loader.trainval_text_data[:split_point]\n",
    "val_text   = meme_loader.trainval_text_data[split_point:]\n",
    "len(train_text), len(val_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# align_model = align_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After 'processor' all tokens are transformed into ids and with paddings.\n",
    "# Preprocessing: resize and crop\n",
    "import os\n",
    "def pre_porcess(text_batch):\n",
    "    '''\n",
    "    text_batch [list]\n",
    "    '''\n",
    "    processor = AlignProcessor.from_pretrained(\"kakaobrain/align-base\")\n",
    "    directory = 'data/memes'\n",
    "    images = []\n",
    "    captions = []\n",
    "    for item in text_batch:\n",
    "        img_path = os.path.join(directory, item['img_fname'])\n",
    "        \n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                img = resize_and_crop_image(img, image_width, image_height)\n",
    "                img_array = np.array(img) # dtype=numpy.uint8\n",
    "                if len(img_array.shape) == 2: # handle gray images with shape of (802, 640)\n",
    "                    images.append(np.stack([img_array, img_array, img_array], axis=-1))\n",
    "                elif img_array.shape[2] == 4: # handle images with shape of (802, 640, 4)\n",
    "                    images.append(img_array[:, :, :-1])\n",
    "                else:\n",
    "                    images.append(img_array)\n",
    "                    # Only load the first meme caption\n",
    "                captions.append(item.get('meme_captions', [\"\"])[0])\n",
    "        except IOError:\n",
    "            print(f\"Error opening image {img_path}\")\n",
    "    return  processor(text=captions, \n",
    "                                    images=images, \n",
    "                                    return_tensors=\"pt\",\n",
    "                                    truncation=True )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused or unrecognized kwargs: truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2033,  4168, 13082,  2667,  2000,  3275,  2041,  2129,  2172,\n",
       "          3637,  2002,  1005,  2222,  2131,  2065,  2027,  3613,  2000,  3422,\n",
       "          2694,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2033,  4168, 13082,  2003,  2667,  2000, 16636,  1996,  9164,\n",
       "          1997,  2111,  2040,  2113,  2037, 12010,  3570,  1998,  2216,  2040,\n",
       "          2024, 12010,  1998,  2024,  2025,  5204,  1012,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2033,  4168, 13082,  2003,  2667,  2000, 16636,  2008,  2045,\n",
       "          2024,  2111,  2008,  2024, 20302,  2055,  2313, 10035,  2066,  2061,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'pixel_values': tensor([[[[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          ...,\n",
       "          [-0.4902, -0.2235, -0.0902,  ...,  0.0431,  0.0510,  0.0431],\n",
       "          [-0.3882, -0.1451, -0.0824,  ...,  0.0275,  0.0275,  0.0510],\n",
       "          [-0.4745, -0.3020, -0.2157,  ...,  0.0275,  0.0196,  0.0431]],\n",
       "\n",
       "         [[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          ...,\n",
       "          [-0.4824, -0.2157, -0.0980,  ..., -0.0667, -0.0588, -0.0588],\n",
       "          [-0.3804, -0.1373, -0.0980,  ..., -0.0824, -0.0824, -0.0588],\n",
       "          [-0.4745, -0.2941, -0.2314,  ..., -0.0824, -0.0902, -0.0588]],\n",
       "\n",
       "         [[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          ...,\n",
       "          [-0.3176, -0.0510,  0.0667,  ..., -0.0667, -0.0588, -0.0588],\n",
       "          [-0.2157,  0.0275,  0.0667,  ..., -0.0824, -0.0824, -0.0588],\n",
       "          [-0.3098, -0.1294, -0.0667,  ..., -0.0824, -0.0902, -0.0588]]],\n",
       "\n",
       "\n",
       "        [[[ 0.8353,  0.8275,  0.8275,  ..., -0.3569, -0.3020, -0.2549],\n",
       "          [ 0.8353,  0.8275,  0.8275,  ..., -0.3490, -0.3020, -0.2471],\n",
       "          [ 0.8353,  0.8275,  0.8275,  ..., -0.3333, -0.3020, -0.2471],\n",
       "          ...,\n",
       "          [ 0.4431,  0.4431,  0.4431,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          [ 0.4431,  0.4431,  0.4431,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          [ 0.4431,  0.4353,  0.4353,  ..., -1.0000, -0.9922, -0.9843]],\n",
       "\n",
       "         [[ 0.6784,  0.6706,  0.6706,  ...,  0.0902,  0.1137,  0.1608],\n",
       "          [ 0.6784,  0.6706,  0.6706,  ...,  0.0902,  0.1216,  0.1686],\n",
       "          [ 0.6784,  0.6706,  0.6706,  ...,  0.0980,  0.1216,  0.1686],\n",
       "          ...,\n",
       "          [-0.0039, -0.0039, -0.0039,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          [-0.0039, -0.0039, -0.0039,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          [-0.0039, -0.0039, -0.0118,  ..., -1.0000, -0.9922, -0.9843]],\n",
       "\n",
       "         [[ 0.4667,  0.4588,  0.4588,  ...,  0.6000,  0.6471,  0.6941],\n",
       "          [ 0.4667,  0.4588,  0.4588,  ...,  0.6000,  0.6471,  0.6941],\n",
       "          [ 0.4510,  0.4588,  0.4588,  ...,  0.6157,  0.6392,  0.6863],\n",
       "          ...,\n",
       "          [-0.1608, -0.1608, -0.1608,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          [-0.1608, -0.1608, -0.1608,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          [-0.1608, -0.1608, -0.1686,  ..., -1.0000, -0.9922, -0.9843]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          ...,\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]],\n",
       "\n",
       "         [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          ...,\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]],\n",
       "\n",
       "         [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          ...,\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]]]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_porcess(train_text[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utilities import recall_at_k\n",
    "from src.evaluation import similarity_align\n",
    "\n",
    "def evaluation(model, dataset=val_text):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        print('Number of samples:', len(dataset))\n",
    "        inputs = pre_porcess(dataset)\n",
    "        # Calculate the similarity matrix of memes and texts.\n",
    "        text2image_si = similarity_align(inputs, model, device)\n",
    "        # t2i R@k\n",
    "        t2i = recall_at_k(text2image_si, prefix='t2i_')\n",
    "        # i2t R@k\n",
    "        i2t = recall_at_k(text2image_si.T, prefix='i2t_')\n",
    "        # Merge two dictionaries\n",
    "        i2t.update(t2i)\n",
    "    # print(i2t)\n",
    "    return i2t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation(align_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, save_dir, filename=\"checkpoint.pth\"):\n",
    "    \"\"\"\n",
    "    Saves a checkpoint of the model, optimizer, and training parameters.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to save.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer used for training.\n",
    "        epoch (int): The current epoch.\n",
    "        save_dir (str): The directory to save the checkpoint in.\n",
    "        filename (str, optional): The filename of the checkpoint. Default: 'checkpoint.pth'.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    checkpoint_path = os.path.join(save_dir, filename)\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "def load_checkpoint(model, optimizer, checkpoint_path):\n",
    "    \"\"\"\n",
    "    Loads a checkpoint and resumes training from the saved state.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to load the checkpoint into.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer to load the checkpoint into.\n",
    "        checkpoint_path (str): The path to the checkpoint file.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint file not found: {checkpoint_path}\")\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch'] + 1  # Add 1 to epoch for next iteration\n",
    "\n",
    "    print(f\"Checkpoint loaded from {checkpoint_path}. Resuming training from epoch {epoch}.\")\n",
    "    return model, optimizer, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, train_data, validation_data, epochs, \n",
    "          optimizer, batch_size, log_step=10, evaluation_step =50, \n",
    "          saving_model_step=50, out_dir='./output'):\n",
    "  \"\"\"\n",
    "  Trains a CNN model with cosine scheduler and Wandb recording.\n",
    "\n",
    "  Args:\n",
    "      model (torch.nn.Module): The CNN model to train.\n",
    "      train_data (torch.utils.data.DataLoader): Training data loader.\n",
    "      val_data (torch.utils.data.DataLoader): Validation data loader (optional).\n",
    "      epochs (int): Number of training epochs.\n",
    "      learning_rate (float): Initial learning rate.\n",
    "      batch_size (int): Batch size for training.\n",
    "      device (str): Device to use for training ('cpu' or 'cuda' if available).\n",
    "  \"\"\"\n",
    "  train_num_batches = math.ceil(len(train_data) / batch_size)\n",
    "  val_num_batches = math.ceil(len(validation_data) / batch_size)\n",
    "  total_steps = epochs * train_num_batches\n",
    "  print(f'train_num_batches: {train_num_batches}, val_num_batches: {val_num_batches}, total_steps: {total_steps}')\n",
    "  # Initialize optimizer and scheduler\n",
    "\n",
    "  scheduler = CosineAnnealingLR(optimizer, T_max=epochs)  # Cosine scheduler ???\n",
    "  step = 0\n",
    "  # Initialize Wandb (optional)\n",
    "  if wandb.run is None:\n",
    "    wandb.init(project=\"meme-text\")  # Replace with your project name\n",
    "\n",
    "  # Training loop\n",
    "  train_loss = 0\n",
    "  for epoch in range(epochs):\n",
    "    for idx in range(0, len(train_data), batch_size):\n",
    "      model.train()\n",
    "      input = pre_porcess(train_data[idx: min(idx+batch_size, len(train_data))])\n",
    "      input.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      output = model(**input)\n",
    "      loss = output['loss']\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      train_loss += loss.item()\n",
    "\n",
    "      # Log training metrics to Wandb (optional)\n",
    "      if step%log_step == (log_step - 1) and wandb.run is not None:\n",
    "        print(f'step: {step}/{total_steps}; training loss: {train_loss/step}')\n",
    "        wandb.log({\"train_loss\": train_loss/step, 'learning_rate': optimizer.param_groups[0][\"lr\"]})\n",
    "        train_loss = 0\n",
    "      # Validation step (optional)\n",
    "      if step%evaluation_step == (evaluation_step - 1) and wandb.run is not None:\n",
    "        metrics = evaluation(model)\n",
    "        print(f'step: {step}/{total_steps}; metrics: {metrics}')\n",
    "        wandb.log(metrics)\n",
    "      if step%saving_model_step == (saving_model_step - 1):\n",
    "        # save the model\n",
    "        now = datetime.now()\n",
    "        dt_string = now.strftime(\"%d_%m_%H-%M-%S\") # 27_12_10-09-20\n",
    "        save_checkpoint(model, optimizer, epoch, out_dir, f'{dt_string}_align_{step}.pth')\n",
    "        pass\n",
    "      step += 1\n",
    "    # Update scheduler after each epoch\n",
    "    scheduler.step()\n",
    "\n",
    "  # Finish Wandb run (optional)\n",
    "  if wandb.run is not None:\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Login to your Wandb account (optional)\n",
    "# wandb.login()\n",
    "# wandb.init(project=\"meme-text\") \n",
    "# learning_rate = 1e-5\n",
    "# optimizer = Adam(align_model.model.parameters(), lr=learning_rate)\n",
    "# train(align_model.model, train_text, val_text, epochs=3, \n",
    "#       optimizer=optimizer, batch_size=8,\n",
    "#       log_step=10, evaluation_step=50, saving_model_step=218)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sweep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 57xm3plb\n",
      "Sweep URL: https://wandb.ai/adl_shilingdeng/meme-text/sweeps/57xm3plb\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "                'method': 'bayes',\n",
    "                'metric': {'goal':'minimize', 'name':'loss'},\n",
    "                'parameters': {\n",
    "                    'epochs': {'values': [1, 2]},\n",
    "                    'learning_rate': {'distribution': 'uniform',\n",
    "                                        'max': 4e-5,\n",
    "                                        'min': 1e-6},\n",
    "                    'optimizer': {'values': ['adam']}\n",
    "                }\n",
    " }\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"meme-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep(config=None):\n",
    "    with wandb.init(project='meme-text', entity='serkar', config=config):\n",
    "        config = wandb.config\n",
    "\n",
    "        # Init the model\n",
    "        align_model = align_base()\n",
    "        align_model.model.to(device)\n",
    "\n",
    "        # # Define optimizer\n",
    "        optimizer = torch.optim.Adam(align_model.model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "        wandb.watch(align_model.model, log=\"all\")\n",
    "\n",
    "        # Run training\n",
    "        train(align_model.model, train_text, val_text, epochs=config.epochs, \n",
    "                optimizer=optimizer, batch_size=6,\n",
    "                log_step=10, evaluation_step=50, saving_model_step=218)\n",
    "        # After training or computations\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: a5ep8inu with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3.645353107667553e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshilingdeng7187\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/shiling/git/meme_text_retrieval/wandb/run-20240527_075130-a5ep8inu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adl_shilingdeng/meme-text/runs/a5ep8inu' target=\"_blank\">snowy-sweep-1</a></strong> to <a href='https://wandb.ai/adl_shilingdeng/meme-text' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/adl_shilingdeng/meme-text/sweeps/57xm3plb' target=\"_blank\">https://wandb.ai/adl_shilingdeng/meme-text/sweeps/57xm3plb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adl_shilingdeng/meme-text' target=\"_blank\">https://wandb.ai/adl_shilingdeng/meme-text</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/adl_shilingdeng/meme-text/sweeps/57xm3plb' target=\"_blank\">https://wandb.ai/adl_shilingdeng/meme-text/sweeps/57xm3plb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adl_shilingdeng/meme-text/runs/a5ep8inu' target=\"_blank\">https://wandb.ai/adl_shilingdeng/meme-text/runs/a5ep8inu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_num_batches: 777, val_num_batches: 195, total_steps: 777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused or unrecognized kwargs: truncation.\n",
      "Unused or unrecognized kwargs: truncation.\n",
      "Unused or unrecognized kwargs: truncation.\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, function=sweep, count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a check point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded from ./output/align_1525_27_05_00-21-48.pth. Resuming training from epoch 2.\n",
      "Number of samples: 559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused or unrecognized kwargs: truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([559, 640])\n",
      "torch.Size([559, 640])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'i2t_r1': 0.49016100178890876,\n",
       " 'i2t_r5': 0.6869409660107334,\n",
       " 'i2t_r10': 0.738819320214669,\n",
       " 'i2t_r_mean': 0.6386404293381037,\n",
       " 't2i_r1': 0.5116279069767442,\n",
       " 't2i_r5': 0.6797853309481217,\n",
       " 't2i_r10': 0.7584973166368515,\n",
       " 't2i_r_mean': 0.6499701848539058}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new_model = align_base()\n",
    "# optimizer = Adam(new_model.model.parameters(), lr=0.1)\n",
    "# load_checkpoint(new_model.model, optimizer, './output/align_1525_27_05_00-21-48.pth')\n",
    "# # val dataset\n",
    "# evaluation(new_model.model)\n",
    "# # test dataset\n",
    "# evaluation(new_model.model, meme_loader.test_text_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard\n",
    "tensorboard --logdir=check_points/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"shutdown -t  +30 \") # in minutes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
